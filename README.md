# FAQ Support Chatbot using RAG
This repository implements a Retrieval-Augmented Generation (RAG) based FAQ support chatbot for an HR SaaS company. It parses a plain text document containing FAQs (>1000 words), chunks it into at least 20 meaningful pieces, generates embeddings, and stores them in a FAISS vector store for efficient retrieval. The query pipeline accepts user questions, performs vector search to retrieve relevant chunks, and uses an LLM to generate accurate answers. Outputs are structured as JSON for transparency, including the user question, system answer, and related chunks. A bonus evaluator scores the answer quality (0-10) with a reason.
The system is built with Python, LangChain for RAG components, FAISS for vector search, and OpenRouter for embeddings and LLM access (compatible with OpenAI models).

## Setup
- Python version: 3.10+ (tested on 3.12).
- Install dependencies: `pip install -r requirements.txt`
- Tested with LangChain 1.0.5. If using older versions, may need adjustments.
- API key setup: Copy `.env.example` to `.env` and set `OPENROUTER_API_KEY=your-key`. Optionally override `EMBEDDING_MODEL` or `LLM_MODEL`.
- Note: Uses OpenRouter API (base_url=https://openrouter.ai/api/v1) for embeddings and LLM, compatible with OpenAI format.

## Usage
- Build the index (data pipeline): `python src/build_index.py`
  - This loads `data/faq_document.txt`, chunks it into >=20 meaningful pieces, generates embeddings, and stores in FAISS index.
  - Outputs: "Index built with X chunks." and saves chunks to `outputs/chunks.json` for inspection.
- Run query pipeline: `python src/query.py "Your question here" [--evaluate]`
  - Example command with sample question: `python src/query.py "How to request vacation?" --evaluate`
  - Outputs: JSON with user_question, system_answer, chunks_related (and score/reason if --evaluate).
- For samples: Run multiple queries and manually append outputs to `outputs/sample_queries.json` as a list.

## Project Structure
- `data/`: Source files.
  - `faq_document.txt`: Plain text document (>1000 words) with HR SaaS FAQs on policies, procedures, features, etc.
- `src/`: Core scripts.
  - `build_index.py`: Data pipeline script - loads document, chunks text (20+ chunks), generates embeddings, saves to storage (FAISS).
  - `query.py`: Query pipeline script - accepts user question as input, converts to embedding, performs vector search (k-NN/ANN/Range/Hybrid), retrieves relevant chunks, generates answer using LLM, returns JSON with user_question, system_answer, chunks_related.
- `outputs/`: Generated files.
  - `sample_queries.json`: At least 3 example query-response pairs in JSON format, demonstrating system works end-to-end.
  - `chunks.json`: Generated by build_index.py - list of chunk texts for review/debugging.
  - `query_log.txt`: Text log of query outputs.
- `.env.example`: Template for environment variables (e.g., OPENROUTER_API_KEY, EMBEDDING_MODEL=text-embedding-3-small).
- `requirements.txt`: Dependencies list.
- `.gitignore`: Ignores sensitive/env files, caches, FAISS index.

## Technical Choices
- **Chunking Method**: RecursiveCharacterTextSplitter from langchain-text-splitters with chunk_size=500, chunk_overlap=100.
  - Why recursive: Better for unstructured text like FAQs than fixed-size splitting; recursively splits on separators (e.g., \n\n, \n, sentences, words) to create coherent chunks that preserve semantic units (e.g., full paragraphs/policies). This avoids breaking mid-sentence, improving embedding quality and retrieval relevance.
  - Why size=500: Balances granularity (small enough for precise retrieval) with context (large enough to include meaningful info without exceeding embedding model limits ~8192 chars for text-embedding-3-small). Results in ~25-30 chunks for our ~1450-word doc, exceeding min 20.
  - Why overlap=100: Provides continuity between chunks, ensuring overlapping context for queries spanning chunk boundaries (e.g., a policy detail split across chunks). 100 chars is ~20-25 words, sufficient without excessive redundancy/storage overhead.
- **Embedding Generation**: OpenAI's text-embedding-3-small via OpenRouter.
  - Why this model: Compact (1536 dims), accurate for general text, cost-effective (~$0.00002/1k tokens). OpenRouter routing allows access with your API key, avoiding direct OpenAI dependency.
  - Why embeddings at all: Convert text to vectors for semantic search, enabling RAG to retrieve based on meaning, not just keywords.
- **Vector Search/Storage**: FAISS with search_type="similarity" (k-NN, k=5). Why? FAISS is fast and scalable for approximate/exact nearest neighbors on CPU. "Similarity" (cosine) ensures top relevant chunks without strict threshold, avoiding empty results.
- **RAG Architecture**: LangChain's RetrievalQA chain.
  - Why LangChain: Provides modular, production-ready components for RAG (loaders, splitters, vectorstores, chains), reducing custom code. Easy integration with OpenRouter via OpenAI-compatible classes.
- **LLM for Generation/Evaluator**: gpt-4o-mini via OpenRouter.
  - Why this model: Fast, cheap (~$0.15/1M tokens input), capable for concise answers. Temperature=0 for deterministic, factual responses.
  - Why evaluator as bonus: LLMChain with custom prompt for scoring (0-10 on relevance/accuracy/completeness); provides QA without human loop, useful for tuning.
- **Storage**: Local FAISS files (in-memory during run, persisted to disk).
  - Why local: Simple for assignment, no cloud costs/setup. Extensible to vector DBs for production.
- **Other Tools/Libs**: python-dotenv for env vars (secure API keys), argparse for CLI (user-friendly queries).
  - Overall: Choices prioritize maintainability (modular code), efficiency (fast local run), and alignment with assignment (RAG concepts, JSON output, >20 chunks).
